---
title: "Redes neuronales"
author: "Angie Rodríguez Duque & César Saavedra Vanegas"
date: "Octubre 30 de 2020"
output:
  ioslides_presentation:
    widescreen: true 
    smaller: true 
    transition: slower
---
```{r warning=FALSE, include=F, paged.print=TRUE}
suppressMessages(library(neuralnet))
suppressMessages(library(NeuralNetTools))
suppressMessages(library(dplyr))
suppressMessages(library(readxl))
suppressMessages(library(tidyverse))
suppressMessages(library(FactoMineR))
suppressMessages(library(factoextra))
suppressMessages(library(foreign))
suppressMessages(library(corrplot))
suppressMessages(library(polycor))
suppressMessages(library(psych))
suppressMessages(library(gplots))
suppressMessages(library(gridExtra))
suppressMessages(library(viridis))
suppressMessages(library(lsr))
suppressMessages(library(DescTools))
suppressMessages(library(magrittr))
suppressMessages(library(nlme))
suppressMessages(library(MASS))
suppressMessages(library(multilevel))
suppressMessages(library(reshape))
suppressMessages(library(homals))
suppressMessages(library(GGally))
suppressMessages(library(CCA))
suppressMessages(library(plotly))
suppressMessages(library(broom))
suppressMessages(library(readr))
suppressMessages(library(readxl))
suppressMessages(library(MASS))
suppressMessages(library(VGAM))
```

# REDES NEURONALES

##
Las redes neuronales artificiales son una representación abstracta del comportamiento de una red neuronal biológica. Su contexto se remonta a 1943, año en el cual McCulloch y Pitts proponen el primer modelo neuronal, dicho modelo era un modelo binario, en el cual cada neurona tenía un escalón o umbral prefijado. De esta manera sirvió de base para los modelos posteriores.


## Capas

Las redes neuronales están compuestas por capas de neuronas que se comunican entre si y es posible dividirlas de la siguiente manera: 

<div class="columns-2">

- Capa de entrada
- Capa oculta
- Capa de salida

![Capas en redes neuronales](3.png "Red neuronal"){width=300px}
<div class="columns-2">

# ADAPTACIÓN DE REDES NEURONALES

## 

<center>
![Estructura](1.png "Red neuronal"){height=400px}

## Función base
<div style="text-align: justify">
**Función lineal de tipo hiperplano:** Es una función de base cuyo valor de red corresponde a una combinación lineal de las entradas.


$$u_{i}(w,x)=\sum_{j=1}^{n}w_{ij}x_{j}$$    


**Función radial de tipo hiperesférico:**

$$u_{i}(w,x)= \sqrt{\sum_{j=1}^{n}(x_{j}-w_{ij})^{2}}$$
<div/>
## Función de activación
<div style="text-align: justify">
Las funciones de activación se dividen en dos tipos como: lineal y no lineal

<center>
![Red neuronal](funciones.jpg "Regresión local"){width=500px}

<div/>

# Algoritmo back propagation
##
<div style="text-align: justify">

1. Asignamos a cada conexión neuronal un peso con un valor pequeño, pero no nulo.

2. Introducimos la primera observación de nuestro conjunto de entrenamiento por la capa inicial de la red neuronal.

3. La información se propaga de izquierda a derecha, activando cada neurona que ahora es afectada por el peso de cada conexión, hasta llegar a la capa de neuronas de salida, obteniendo el resultado final para esa observación en concreto.

4. Medimos el error que hemos cometido para esa observación.

5. Comienza la propagación hacia atrás de derecha a izquierda, actualizando los pesos de cada conexión neuronal, dependiendo de la responsabilidad del peso actualizado en el error cometido.

6. Repetimos los pasos desde el paso 2, actualizando todos los pesos para cada observación o conjunto de observaciones de nuestro conjunto de entrenamiento.

7. Cuando todas las observaciones del conjunto de entrenamiento ha pasado por la red neuronal, hemos completado lo que se denomina un Epoch. Podemos realizar tantos Epochs como creamos convenientes.

<div/>

# ALGUNOS PROBLEMAS EN EL ENTRENAMIENTO DE LAS REDES NEURONALES
##
<div style="text-align: justify">

+ Valores iniciales:
+ Sobreajuste:
+ Escalado de las entradas:
+ Número de capas y unidades ocultas

<div/>

# APLICACIONES
##
<div style="text-align: justify">

La mayoría de las aplicaciones de las redes neuronales consisten en:

**Finanzas:** Previsión de la evolución de los precios, Valoración del riesgo de los créditos, Identificación de falsificaciones, Interpretación de firmas. 

**Manufacturación:** Robots automatizados y sistemas de control (visión artificial y sensores de presión, temperatura, gas, etc.), Inspección de la calidad. 

**Militares:** Clasificación de las señales de radar y Reconocimiento y seguimiento en el tiro al blanco. 

<div/>

# EJEMPLO DE APLICACIÓN
##
<div style="text-align: justify">

### **El conjunto de datos**

Se hará uso del conjunto de datos denominado: "Boston" perteneciente al paquete MASS. El conjunto de datos de Boston es una colección de datos sobre el valor de las viviendas en los suburbios de Boston. Nuestro objetivo es predecir el valor medio de las viviendas ocupadas por sus propietarios (medv) utilizando todas las demás variables continuas disponibles.

```{r warning=FALSE, include=T, paged.print=TRUE}
set.seed(500)
library(MASS)
data <- Boston
```

Como primera medida se verifica que no falte ningún punto de datos; de lo contrario, es necesario corregir el conjunto.

```{r warning=FALSE, include=T, paged.print=TRUE}
apply(data,2,function(x) sum(is.na(x)))
```
<div/>

##

```{r warning=FALSE, include=T, paged.print=F}
index <- sample(1:nrow(data),round(0.75*nrow(data)))
train <- data[index,]
test <- data[-index,]
lm.fit <- glm(medv~., data=train)
summary(lm.fit)
pr.lm <- predict(lm.fit,test)
MSE.lm <- sum((pr.lm - test$medv)^2)/nrow(test)
```




##

### **Adaptación de la red neuronal**

Antes de instalar una red neuronal, es necesario realizar algunos preparativos. Como primer paso, se aborda el preprocesamiento de datos. Aquí se normalizan los datos antes de entrenar la red neuronal.

Se elige el método min-max y se escalan los datos en el intervalo [0,1]. Normalmente, el escalado en los intervalos [0,1] o [-1,1] tiende a dar mejores resultados. Por lo tanto, escalamos y dividimos los datos antes de continuar:


```{r warning=FALSE, include=T, paged.print=TRUE}
maxs <- apply(data, 2, max) 
mins <- apply(data, 2, min)
scaled <- as.data.frame(scale(data, center = mins, scale = maxs - mins))
train_ <- scaled[index,]
test_ <- scaled[-index,]
```

+ **Nota:** Tener en cuenta que *scale* devuelve una matriz que necesita ser convertida en un data.frame.

## 

### **Parámetros**

<div style="text-align: justify">

En esta ocasión, vamos a utilizar 2 capas ocultas con esta configuración: 

$$13: 5: 3: 1$$

La capa de entrada tiene 13 entradas, las dos capas ocultas tienen 5 y 3 neuronas y la capa de salida tiene, por supuesto, una única salida ya que estamos haciendo regresión.

```{r warning=FALSE, include=T, paged.print=TRUE}
library(neuralnet)
n <- names(train_)
f <- as.formula(paste("medv ~", paste(n[!n %in% "medv"], collapse = " + ")))
nn <- neuralnet(f,data=train_,hidden=c(5,3),linear.output=T)
```

<div/>

## 

### **Representación gráfica**

<div style="text-align: justify">

El paquete neuralnet proporciona una buena herramienta para trazar el modelo. Esta es la representación gráfica del modelo con los pesos en cada conexión:

```{r warning=FALSE, include=T, paged.print=TRUE}
plot(nn)
```

<div/>


## 

### **Observaciones**

<div style="text-align: justify">

+ Las líneas negras muestran las conexiones entre cada capa y los pesos en cada conexión.

+ Las líneas azules muestran el término de sesgo agregado en cada paso.

+ El sesgo se puede pensar como la intersección de un modelo lineal.

+ Finalmente, el algoritmo de entrenamiento ha convergido y, por lo tanto, el modelo está listo para ser utilizado.


<div/>

